{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFT712 Projet Fin de Session\n",
    "\n",
    "------------------------------\n",
    "MAHDI AIT LHAJ LOUTFI (aitm2302)  \n",
    "YOVAN TURCOTTE (tury1903)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des données et analyse des résultats :\n",
    "Dans ce Notebook, on va analyse les resultas de l'entrainement. Dans le fond on va choisir le modele avec la meilleur performance, et le tester sur les donnees de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "#-------for Pre-Processing----\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler)\n",
    "from src.DataProcesser.Processer import DataProcesser\n",
    "#------------------------------\n",
    "import src.Manager\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les données :\n",
    "Si la cellule suivante marche, ca veut dire que l'extraction des données marche tres bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total dataset has 990 observations with dimentionality 192.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>margin10</th>\n",
       "      <th>margin11</th>\n",
       "      <th>margin12</th>\n",
       "      <th>margin13</th>\n",
       "      <th>margin14</th>\n",
       "      <th>margin15</th>\n",
       "      <th>margin16</th>\n",
       "      <th>margin17</th>\n",
       "      <th>margin18</th>\n",
       "      <th>margin19</th>\n",
       "      <th>margin20</th>\n",
       "      <th>margin21</th>\n",
       "      <th>margin22</th>\n",
       "      <th>margin23</th>\n",
       "      <th>margin24</th>\n",
       "      <th>margin25</th>\n",
       "      <th>margin26</th>\n",
       "      <th>margin27</th>\n",
       "      <th>margin28</th>\n",
       "      <th>margin29</th>\n",
       "      <th>margin30</th>\n",
       "      <th>margin31</th>\n",
       "      <th>margin32</th>\n",
       "      <th>margin33</th>\n",
       "      <th>margin34</th>\n",
       "      <th>margin35</th>\n",
       "      <th>margin36</th>\n",
       "      <th>margin37</th>\n",
       "      <th>margin38</th>\n",
       "      <th>margin39</th>\n",
       "      <th>margin40</th>\n",
       "      <th>margin41</th>\n",
       "      <th>margin42</th>\n",
       "      <th>margin43</th>\n",
       "      <th>margin44</th>\n",
       "      <th>margin45</th>\n",
       "      <th>margin46</th>\n",
       "      <th>margin47</th>\n",
       "      <th>margin48</th>\n",
       "      <th>margin49</th>\n",
       "      <th>margin50</th>\n",
       "      <th>margin51</th>\n",
       "      <th>margin52</th>\n",
       "      <th>margin53</th>\n",
       "      <th>margin54</th>\n",
       "      <th>margin55</th>\n",
       "      <th>margin56</th>\n",
       "      <th>margin57</th>\n",
       "      <th>margin58</th>\n",
       "      <th>margin59</th>\n",
       "      <th>margin60</th>\n",
       "      <th>margin61</th>\n",
       "      <th>margin62</th>\n",
       "      <th>margin63</th>\n",
       "      <th>margin64</th>\n",
       "      <th>shape1</th>\n",
       "      <th>shape2</th>\n",
       "      <th>shape3</th>\n",
       "      <th>shape4</th>\n",
       "      <th>shape5</th>\n",
       "      <th>shape6</th>\n",
       "      <th>shape7</th>\n",
       "      <th>shape8</th>\n",
       "      <th>shape9</th>\n",
       "      <th>shape10</th>\n",
       "      <th>shape11</th>\n",
       "      <th>shape12</th>\n",
       "      <th>shape13</th>\n",
       "      <th>shape14</th>\n",
       "      <th>shape15</th>\n",
       "      <th>shape16</th>\n",
       "      <th>shape17</th>\n",
       "      <th>shape18</th>\n",
       "      <th>shape19</th>\n",
       "      <th>shape20</th>\n",
       "      <th>shape21</th>\n",
       "      <th>shape22</th>\n",
       "      <th>shape23</th>\n",
       "      <th>shape24</th>\n",
       "      <th>shape25</th>\n",
       "      <th>shape26</th>\n",
       "      <th>shape27</th>\n",
       "      <th>shape28</th>\n",
       "      <th>shape29</th>\n",
       "      <th>shape30</th>\n",
       "      <th>shape31</th>\n",
       "      <th>shape32</th>\n",
       "      <th>shape33</th>\n",
       "      <th>shape34</th>\n",
       "      <th>shape35</th>\n",
       "      <th>shape36</th>\n",
       "      <th>shape37</th>\n",
       "      <th>shape38</th>\n",
       "      <th>shape39</th>\n",
       "      <th>shape40</th>\n",
       "      <th>shape41</th>\n",
       "      <th>shape42</th>\n",
       "      <th>shape43</th>\n",
       "      <th>shape44</th>\n",
       "      <th>shape45</th>\n",
       "      <th>shape46</th>\n",
       "      <th>shape47</th>\n",
       "      <th>shape48</th>\n",
       "      <th>shape49</th>\n",
       "      <th>shape50</th>\n",
       "      <th>shape51</th>\n",
       "      <th>shape52</th>\n",
       "      <th>shape53</th>\n",
       "      <th>shape54</th>\n",
       "      <th>shape55</th>\n",
       "      <th>shape56</th>\n",
       "      <th>shape57</th>\n",
       "      <th>shape58</th>\n",
       "      <th>shape59</th>\n",
       "      <th>shape60</th>\n",
       "      <th>shape61</th>\n",
       "      <th>shape62</th>\n",
       "      <th>shape63</th>\n",
       "      <th>shape64</th>\n",
       "      <th>texture1</th>\n",
       "      <th>texture2</th>\n",
       "      <th>texture3</th>\n",
       "      <th>texture4</th>\n",
       "      <th>texture5</th>\n",
       "      <th>texture6</th>\n",
       "      <th>texture7</th>\n",
       "      <th>texture8</th>\n",
       "      <th>texture9</th>\n",
       "      <th>texture10</th>\n",
       "      <th>texture11</th>\n",
       "      <th>texture12</th>\n",
       "      <th>texture13</th>\n",
       "      <th>texture14</th>\n",
       "      <th>texture15</th>\n",
       "      <th>texture16</th>\n",
       "      <th>texture17</th>\n",
       "      <th>texture18</th>\n",
       "      <th>texture19</th>\n",
       "      <th>texture20</th>\n",
       "      <th>texture21</th>\n",
       "      <th>texture22</th>\n",
       "      <th>texture23</th>\n",
       "      <th>texture24</th>\n",
       "      <th>texture25</th>\n",
       "      <th>texture26</th>\n",
       "      <th>texture27</th>\n",
       "      <th>texture28</th>\n",
       "      <th>texture29</th>\n",
       "      <th>texture30</th>\n",
       "      <th>texture31</th>\n",
       "      <th>texture32</th>\n",
       "      <th>texture33</th>\n",
       "      <th>texture34</th>\n",
       "      <th>texture35</th>\n",
       "      <th>texture36</th>\n",
       "      <th>texture37</th>\n",
       "      <th>texture38</th>\n",
       "      <th>texture39</th>\n",
       "      <th>texture40</th>\n",
       "      <th>texture41</th>\n",
       "      <th>texture42</th>\n",
       "      <th>texture43</th>\n",
       "      <th>texture44</th>\n",
       "      <th>texture45</th>\n",
       "      <th>texture46</th>\n",
       "      <th>texture47</th>\n",
       "      <th>texture48</th>\n",
       "      <th>texture49</th>\n",
       "      <th>texture50</th>\n",
       "      <th>texture51</th>\n",
       "      <th>texture52</th>\n",
       "      <th>texture53</th>\n",
       "      <th>texture54</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.104490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.109380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052734</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.124020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.126950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.083984</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.132810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     margin1   margin2   margin3   margin4   margin5   margin6   margin7  margin8   margin9  margin10  margin11  margin12  margin13  margin14  \\\n",
       "id                                                                                                                                              \n",
       "1   0.007812  0.023438  0.023438  0.003906  0.011719  0.009766  0.027344  0.0      0.001953  0.033203  0.013672  0.019531  0.066406  0.000000   \n",
       "2   0.005859  0.000000  0.031250  0.015625  0.025391  0.001953  0.019531  0.0      0.000000  0.007812  0.003906  0.027344  0.023438  0.000000   \n",
       "3   0.005859  0.009766  0.019531  0.007812  0.003906  0.005859  0.068359  0.0      0.000000  0.044922  0.007812  0.011719  0.021484  0.001953   \n",
       "5   0.000000  0.003906  0.023438  0.005859  0.021484  0.019531  0.023438  0.0      0.013672  0.017578  0.001953  0.019531  0.001953  0.003906   \n",
       "6   0.005859  0.003906  0.048828  0.009766  0.013672  0.015625  0.005859  0.0      0.000000  0.005859  0.001953  0.044922  0.041016  0.011719   \n",
       "\n",
       "    margin15  margin16  margin17  margin18  margin19  margin20  margin21  margin22  margin23  margin24  margin25  margin26  margin27  margin28  \\\n",
       "id                                                                                                                                               \n",
       "1   0.029297  0.0       0.031250  0.011719  0.000000  0.025391  0.023438  0.001953  0.0       0.015625  0.000000  0.031250  0.000000  0.013672   \n",
       "2   0.033203  0.0       0.009766  0.009766  0.007812  0.007812  0.019531  0.007812  0.0       0.000000  0.007812  0.027344  0.003906  0.037109   \n",
       "3   0.025391  0.0       0.009766  0.011719  0.007812  0.005859  0.041016  0.001953  0.0       0.015625  0.000000  0.009766  0.001953  0.009766   \n",
       "5   0.035156  0.0       0.005859  0.000000  0.001953  0.003906  0.039062  0.009766  0.0       0.009766  0.005859  0.027344  0.001953  0.017578   \n",
       "6   0.041016  0.0       0.009766  0.015625  0.011719  0.007812  0.021484  0.000000  0.0       0.000000  0.015625  0.009766  0.001953  0.017578   \n",
       "\n",
       "    margin29  margin30  margin31  margin32  margin33  margin34  margin35  margin36  margin37  margin38  margin39  margin40  margin41  margin42  \\\n",
       "id                                                                                                                                               \n",
       "1   0.029297  0.015625  0.011719  0.003906  0.025391  0.000000  0.001953  0.011719  0.009766  0.041016  0.037109  0.019531  0.000000  0.009766   \n",
       "2   0.007812  0.048828  0.054688  0.027344  0.003906  0.000000  0.000000  0.003906  0.013672  0.033203  0.033203  0.019531  0.031250  0.009766   \n",
       "3   0.009766  0.015625  0.005859  0.000000  0.017578  0.007812  0.005859  0.009766  0.019531  0.042969  0.021484  0.001953  0.000000  0.005859   \n",
       "5   0.003906  0.021484  0.027344  0.027344  0.000000  0.003906  0.011719  0.003906  0.005859  0.044922  0.056641  0.027344  0.015625  0.007812   \n",
       "6   0.041016  0.005859  0.021484  0.013672  0.017578  0.000000  0.005859  0.009766  0.027344  0.083984  0.027344  0.027344  0.001953  0.001953   \n",
       "\n",
       "    margin43  margin44  margin45  margin46  margin47  margin48  margin49  margin50  margin51  margin52  margin53  margin54  margin55  margin56  \\\n",
       "id                                                                                                                                               \n",
       "1   0.021484  0.015625  0.007812  0.013672  0.027344  0.062500  0.000000  0.017578  0.031250  0.0       0.044922  0.007812  0.025391  0.003906   \n",
       "2   0.007812  0.031250  0.001953  0.039062  0.029297  0.031250  0.035156  0.000000  0.007812  0.0       0.046875  0.046875  0.029297  0.009766   \n",
       "3   0.015625  0.009766  0.011719  0.011719  0.031250  0.109380  0.000000  0.046875  0.031250  0.0       0.035156  0.000000  0.105470  0.000000   \n",
       "5   0.017578  0.029297  0.001953  0.042969  0.085938  0.037109  0.029297  0.007812  0.013672  0.0       0.052734  0.031250  0.037109  0.017578   \n",
       "6   0.017578  0.007812  0.009766  0.011719  0.080078  0.039062  0.009766  0.000000  0.029297  0.0       0.023438  0.021484  0.003906  0.017578   \n",
       "\n",
       "    margin57  margin58  margin59  margin60  margin61  margin62  margin63  margin64    shape1    shape2    shape3    shape4    shape5    shape6  \\\n",
       "id                                                                                                                                               \n",
       "1   0.013672  0.015625  0.013672  0.003906  0.005859  0.003906  0.019531  0.001953  0.000647  0.000609  0.000576  0.000553  0.000516  0.000496   \n",
       "2   0.017578  0.007812  0.013672  0.019531  0.000000  0.000000  0.003906  0.000000  0.000749  0.000695  0.000720  0.000709  0.000688  0.000660   \n",
       "3   0.015625  0.015625  0.019531  0.001953  0.000000  0.005859  0.011719  0.007812  0.000973  0.000910  0.000870  0.000826  0.000796  0.000763   \n",
       "5   0.003906  0.000000  0.001953  0.003906  0.003906  0.000000  0.003906  0.005859  0.000453  0.000465  0.000473  0.000483  0.000492  0.000493   \n",
       "6   0.015625  0.001953  0.041016  0.005859  0.000000  0.000000  0.017578  0.000000  0.000682  0.000598  0.000509  0.000385  0.000299  0.000330   \n",
       "\n",
       "      shape7    shape8    shape9   shape10   shape11   shape12   shape13   shape14   shape15   shape16   shape17   shape18   shape19   shape20  \\\n",
       "id                                                                                                                                               \n",
       "1   0.000474  0.000453  0.000454  0.000429  0.000418  0.000410  0.000406  0.000398  0.000399  0.000389  0.000399  0.000416  0.000414  0.000430   \n",
       "2   0.000624  0.000585  0.000556  0.000531  0.000506  0.000484  0.000467  0.000453  0.000442  0.000435  0.000435  0.000433  0.000443  0.000460   \n",
       "3   0.000729  0.000702  0.000664  0.000629  0.000598  0.000570  0.000546  0.000526  0.000500  0.000494  0.000484  0.000490  0.000491  0.000507   \n",
       "5   0.000492  0.000492  0.000496  0.000497  0.000475  0.000450  0.000428  0.000401  0.000370  0.000342  0.000328  0.000324  0.000368  0.000404   \n",
       "6   0.000427  0.000536  0.000641  0.000645  0.000597  0.000581  0.000620  0.000709  0.000753  0.000839  0.000935  0.001014  0.001022  0.001110   \n",
       "\n",
       "     shape21   shape22   shape23   shape24   shape25   shape26   shape27   shape28   shape29   shape30   shape31   shape32   shape33   shape34  \\\n",
       "id                                                                                                                                               \n",
       "1   0.000422  0.000437  0.000452  0.000467  0.000470  0.000484  0.000489  0.000490  0.000475  0.000486  0.000484  0.000500  0.000513  0.000511   \n",
       "2   0.000471  0.000486  0.000512  0.000538  0.000567  0.000605  0.000644  0.000687  0.000731  0.000777  0.000832  0.000884  0.000902  0.000862   \n",
       "3   0.000525  0.000563  0.000604  0.000663  0.000708  0.000763  0.000824  0.000886  0.000956  0.001030  0.001102  0.001178  0.001193  0.001118   \n",
       "5   0.000350  0.000355  0.000402  0.000454  0.000484  0.000498  0.000493  0.000485  0.000476  0.000479  0.000486  0.000487  0.000497  0.000501   \n",
       "6   0.001218  0.001142  0.001030  0.000961  0.000970  0.000884  0.000816  0.000745  0.000665  0.000691  0.000669  0.000594  0.000486  0.000415   \n",
       "\n",
       "     shape35   shape36   shape37   shape38   shape39   shape40   shape41   shape42   shape43   shape44   shape45   shape46   shape47   shape48  \\\n",
       "id                                                                                                                                               \n",
       "1   0.000519  0.000513  0.000502  0.000498  0.000487  0.000471  0.000458  0.000440  0.000436  0.000427  0.000394  0.000385  0.000382  0.000370   \n",
       "2   0.000842  0.000783  0.000731  0.000679  0.000639  0.000596  0.000560  0.000528  0.000496  0.000474  0.000462  0.000447  0.000434  0.000433   \n",
       "3   0.001049  0.000982  0.000919  0.000862  0.000809  0.000761  0.000702  0.000654  0.000614  0.000574  0.000549  0.000524  0.000511  0.000494   \n",
       "5   0.000490  0.000481  0.000473  0.000463  0.000444  0.000435  0.000435  0.000421  0.000418  0.000410  0.000409  0.000398  0.000410  0.000403   \n",
       "6   0.000347  0.000372  0.000475  0.000508  0.000577  0.000641  0.000639  0.000519  0.000483  0.000600  0.000715  0.000802  0.000887  0.000818   \n",
       "\n",
       "     shape49   shape50   shape51   shape52   shape53   shape54   shape55   shape56   shape57   shape58   shape59   shape60   shape61   shape62  \\\n",
       "id                                                                                                                                               \n",
       "1   0.000367  0.000373  0.000386  0.000389  0.000391  0.000414  0.000422  0.000434  0.000452  0.000471  0.000485  0.000512  0.000536  0.000553   \n",
       "2   0.000435  0.000436  0.000447  0.000464  0.000479  0.000496  0.000518  0.000546  0.000574  0.000608  0.000641  0.000674  0.000703  0.000707   \n",
       "3   0.000495  0.000497  0.000510  0.000533  0.000552  0.000587  0.000610  0.000639  0.000670  0.000697  0.000739  0.000789  0.000836  0.000882   \n",
       "5   0.000424  0.000417  0.000421  0.000434  0.000469  0.000520  0.000529  0.000480  0.000449  0.000437  0.000437  0.000430  0.000429  0.000431   \n",
       "6   0.000792  0.000909  0.001015  0.001044  0.001065  0.000999  0.000973  0.000863  0.000890  0.000911  0.000814  0.000683  0.000593  0.000622   \n",
       "\n",
       "     shape63   shape64  texture1  texture2  texture3  texture4  texture5  texture6  texture7  texture8  texture9  texture10  texture11  texture12  \\\n",
       "id                                                                                                                                                  \n",
       "1   0.000610  0.000661  0.049805  0.017578  0.003906  0.024414  0.001953  0.010742  0.035156  0.007812  0.039062  0.062500   0.000000   0.000000    \n",
       "2   0.000688  0.000747  0.000000  0.000000  0.007812  0.079102  0.000000  0.039062  0.000977  0.000000  0.027344  0.003906   0.000000   0.000000    \n",
       "3   0.000911  0.000971  0.003906  0.047852  0.008789  0.000000  0.097656  0.005859  0.003906  0.101560  0.032227  0.000000   0.000000   0.066406    \n",
       "5   0.000438  0.000443  0.023438  0.000977  0.007812  0.020508  0.000000  0.000000  0.013672  0.004883  0.006836  0.095703   0.015625   0.000000    \n",
       "6   0.000735  0.000755  0.039062  0.036133  0.003906  0.003906  0.046875  0.000000  0.041016  0.041016  0.000000  0.010742   0.000000   0.018555    \n",
       "\n",
       "    texture13  texture14  texture15  texture16  texture17  texture18  texture19  texture20  texture21  texture22  texture23  texture24  texture25  \\\n",
       "id                                                                                                                                                  \n",
       "1   0.007812   0.007812   0.0        0.0        0.047852   0.0        0.054688   0.022461   0.0        0.000977   0.018555   0.001953   0.008789    \n",
       "2   0.014648   0.041016   0.0        0.0        0.003906   0.0        0.020508   0.006836   0.0        0.001953   0.026367   0.020508   0.050781    \n",
       "3   0.007812   0.006836   0.0        0.0        0.057617   0.0        0.000000   0.027344   0.0        0.000000   0.070312   0.000000   0.000977    \n",
       "5   0.000000   0.009766   0.0        0.0        0.000000   0.0        0.034180   0.012695   0.0        0.033203   0.002930   0.000000   0.005859    \n",
       "6   0.000000   0.000000   0.0        0.0        0.001953   0.0        0.006836   0.008789   0.0        0.000000   0.036133   0.000000   0.000000    \n",
       "\n",
       "    texture26  texture27  texture28  texture29  texture30  texture31  texture32  texture33  texture34  texture35  texture36  texture37  texture38  \\\n",
       "id                                                                                                                                                  \n",
       "1   0.015625   0.044922   0.000000   0.037109   0.012695   0.028320   0.000000   0.019531   0.026367   0.005859   0.0        0.004883   0.016602    \n",
       "2   0.001953   0.021484   0.003906   0.027344   0.023438   0.062500   0.000000   0.038086   0.000000   0.019531   0.0        0.001953   0.003906    \n",
       "3   0.000000   0.000977   0.003906   0.035156   0.015625   0.027344   0.000000   0.000000   0.008789   0.015625   0.0        0.000000   0.011719    \n",
       "5   0.019531   0.124020   0.000000   0.017578   0.000977   0.016602   0.009766   0.006836   0.000977   0.005859   0.0        0.000977   0.001953    \n",
       "6   0.000000   0.040039   0.005859   0.005859   0.000000   0.003906   0.000000   0.000000   0.110350   0.000000   0.0        0.000000   0.087891    \n",
       "\n",
       "    texture39  texture40  texture41  texture42  texture43  texture44  texture45  texture46  texture47  texture48  texture49  texture50  texture51  \\\n",
       "id                                                                                                                                                  \n",
       "1   0.034180   0.056641   0.006836   0.000977   0.022461   0.037109   0.004883   0.021484   0.035156   0.000977   0.004883   0.015625   0.000000    \n",
       "2   0.015625   0.004883   0.104490   0.000000   0.061523   0.007812   0.008789   0.013672   0.011719   0.001953   0.035156   0.007812   0.000000    \n",
       "3   0.000977   0.000977   0.000000   0.000000   0.000000   0.005859   0.022461   0.020508   0.021484   0.056641   0.010742   0.008789   0.000000    \n",
       "5   0.096680   0.016602   0.126950   0.000000   0.036133   0.055664   0.000000   0.001953   0.027344   0.000000   0.006836   0.017578   0.004883    \n",
       "6   0.023438   0.007812   0.000000   0.000000   0.008789   0.017578   0.021484   0.061523   0.026367   0.132810   0.000000   0.000000   0.000000    \n",
       "\n",
       "    texture52  texture53  texture54  texture55  texture56  texture57  texture58  texture59  texture60  texture61  texture62  texture63  texture64  \n",
       "id                                                                                                                                                 \n",
       "1   0.0        0.006836   0.037109   0.007812   0.000000   0.002930   0.002930   0.035156   0.0        0.0        0.004883   0.000000   0.025391   \n",
       "2   0.0        0.053711   0.036133   0.000977   0.000000   0.000000   0.000977   0.023438   0.0        0.0        0.000977   0.039062   0.022461   \n",
       "3   0.0        0.000977   0.000000   0.154300   0.000000   0.005859   0.000977   0.007812   0.0        0.0        0.000000   0.020508   0.002930   \n",
       "5   0.0        0.006836   0.022461   0.000000   0.000977   0.000000   0.000000   0.020508   0.0        0.0        0.017578   0.000000   0.047852   \n",
       "6   0.0        0.000000   0.001953   0.096680   0.000000   0.021484   0.000000   0.000000   0.0        0.0        0.000000   0.000000   0.031250   "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DP = DataProcesser(seed = 16082604)\n",
    "DP.importData(label_name = 'species')\n",
    "print(f\"The total dataset has {DP.df().shape[0]} observations with dimentionality {DP.df().shape[1]}.\")\n",
    "DP.df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organisation et distribution des données :\n",
    "1- On va premierement diviser les données d'entrainement,test et validation\n",
    "\n",
    "2- Faire un Sanity Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train + Validation dataset created\n"
     ]
    }
   ],
   "source": [
    "DP.split_data(test_ratio = 0.1)\n",
    "print(\"Train + Validation dataset created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Verified!\n"
     ]
    }
   ],
   "source": [
    "#Sanity Check\n",
    "#Verifier que toutes les classes du test set figurent dans les train + validation sets\n",
    "Diff = set(DP.labels_Test()) - set(DP.labels_Train()) #should be empty set\n",
    "if(Diff == set()):\n",
    "    print(\"Sanity Check Verified!\")\n",
    "else:\n",
    "    print(\"Sanity Check Verification Failed!\")\n",
    "    raise ValueError(\"There are no training observations for the classes : \"+ str(Diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6562 different models.\n"
     ]
    }
   ],
   "source": [
    "with open('../results/results.json') as f:\n",
    "    results = json.load(f)\n",
    "print(f'There are {len(results)} different models.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-259-165c7ca5f063>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precision = np.float(results[key]['results']['Precision'])\n",
      "<ipython-input-259-165c7ca5f063>:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  recall = np.float(results[key]['results']['Recall'])\n"
     ]
    }
   ],
   "source": [
    "#Calculating F1Score\n",
    "for key in results:\n",
    "    precision = np.float(results[key]['results']['Precision'])\n",
    "    recall = np.float(results[key]['results']['Recall'])\n",
    "    if(recall == 0.0 or precision == 0.0):\n",
    "        results[key]['results']['F1Score'] = '0.000'\n",
    "    else:\n",
    "        results[key]['results']['F1Score'] = str(2.0 / (1.0 / precision + 1.0 / recall))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chercher le meilleur modele pour chaque classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classifier types are: {'SVM', 'LogisticRegressionModel', 'KernelModel', 'GenerativeModel', 'NeuralNetwork', 'Perceptron'}\n"
     ]
    }
   ],
   "source": [
    "# Unique Classifiers type\n",
    "classifiers = set(map(lambda x: results[x]['pipeline']['ClassificationParams']['classifier'],results))\n",
    "\n",
    "#Get results for each classifier\n",
    "classifierResults = {classifier:[] for classifier in classifiers}\n",
    "for result in results.values():\n",
    "    classifier = result['pipeline']['ClassificationParams']['classifier']\n",
    "    classifierResults[classifier].append(result)\n",
    "    \n",
    "print('The classifier types are:', classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each classifier type, sort by best results\n",
    "sortedClassifiersBestAccuracy = {classifier:sorted( classifierResults[classifier], \n",
    "                                                    key = lambda x: x['results']['Accuracy'],\n",
    "                                                    reverse = True )[0] for classifier in classifiers}\n",
    "sortedClassifiersBestRecall   = {classifier:sorted( classifierResults[classifier], \n",
    "                                                    key = lambda x: x['results']['Recall'],\n",
    "                                                    reverse = True )[0] for classifier in classifiers}\n",
    "sortedClassifiersBestPrecision= {classifier:sorted( classifierResults[classifier], \n",
    "                                                    key = lambda x: x['results']['Precision'],\n",
    "                                                    reverse = True )[0] for classifier in classifiers}\n",
    "sortedClassifiersBestF1Score   = {classifier:sorted( classifierResults[classifier], \n",
    "                                                    key = lambda x: x['results']['F1Score'],\n",
    "                                                    reverse = True )[0] for classifier in classifiers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprintBestResults(bestModels,metric):\n",
    "    \"\"\"\n",
    "    Method that will nicely display the results\n",
    "    \n",
    "    \n",
    "    bestModels: best model for each classifier type\n",
    "    metric: str of Metric to be used.\n",
    "    \"\"\"\n",
    "    classifiers = set(bestModels)\n",
    "    DataManagerParams    = {classifier:bestModels[classifier]['pipeline']['DataPreProcessingParams']['cases'] \n",
    "                                                                                     for classifier in classifiers}\n",
    "    ClassificationParams = {classifier:bestModels[classifier]['pipeline']['ClassificationParams'] \n",
    "                                                                                     for classifier in classifiers}\n",
    "    metricResults = {classifier:bestModels[classifier]['results'][metric] for classifier in classifiers} \n",
    "    columns = ['PreProcessing1', 'PreProcessing2', 'Hyperparams', metric]\n",
    "    df = pd.DataFrame(np.full((len(classifiers), 4), np.nan), columns = columns, index = classifiers)\n",
    "    for classifier in sorted(metricResults,key = lambda x: metricResults[x], reverse = True):\n",
    "        out = f\"For {classifier}, the model with the best {metric} of {metricResults[classifier]}\\n\\thas hyperparameters: \"\n",
    "        hyperparams = ClassificationParams[classifier] .copy()\n",
    "        del hyperparams['classifier']\n",
    "        hyperparams = [f'{hp}={str(hyperparams[hp])[:8]}' for hp in hyperparams]\n",
    "        preprocessing = [preprocess['method'] for preprocess in DataManagerParams[classifier]]\n",
    "        \n",
    "        \n",
    "        df.loc[classifier] = np.array([preprocessing[0],preprocessing[1],hyperparams,metricResults[classifier] ],dtype=object)\n",
    "        df.sort_values(metric,axis=0, inplace=True, ascending = False)\n",
    "    \n",
    "    #Printing\n",
    "    print('\\t\\t**RESULTS**')\n",
    "    print(df.drop('Hyperparams',axis=1))\n",
    "    print('\\n\\t\\t**HYPERPARAMS**')\n",
    "    print(df['Hyperparams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t**RESULTS**\n",
      "                         PreProcessing1 PreProcessing2 Accuracy\n",
      "KernelModel              StandardScaler  LDA            0.984  \n",
      "SVM                      Normalize       LDA            0.978  \n",
      "LogisticRegressionModel  Normalize       LDA            0.978  \n",
      "NeuralNetwork            Normalize       LDA            0.977  \n",
      "Perceptron               StandardScaler  LDA            0.927  \n",
      "GenerativeModel          StandardScaler  LDA            0.811  \n",
      "\n",
      "\t\t**HYPERPARAMS**\n",
      "KernelModel                [alpha=1e-09, kernel=rbf, gamma=0.002311]                                                                    \n",
      "SVM                        [C=10000.0, kernel=sigmoid, degree=2, gamma=0.067990]                                                        \n",
      "LogisticRegressionModel    [solver=liblinea, random_state=0, penalty=l2, tol=0.0001, C=78.47599]                                        \n",
      "NeuralNetwork              [hidden_layer_sizes=100, activation=relu, solver=adam, alpha=8.653013, learning_rate=invscali, max_iter=1000]\n",
      "Perceptron                 [loss=perceptr, penalty=l2, alpha=0.022025, learning_rate=invscali, eta0=1]                                  \n",
      "GenerativeModel            []                                                                                                           \n",
      "Name: Hyperparams, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pprintBestResults(sortedClassifiersBestAccuracy, 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t**RESULTS**\n",
      "                         PreProcessing1 PreProcessing2 Precision\n",
      "KernelModel              Normalize       LDA            0.986   \n",
      "SVM                      Normalize       LDA            0.982   \n",
      "NeuralNetwork            Normalize       LDA            0.980   \n",
      "LogisticRegressionModel  Normalize       LDA            0.979   \n",
      "Perceptron               StandardScaler  LDA            0.936   \n",
      "GenerativeModel          StandardScaler  LDA            0.893   \n",
      "\n",
      "\t\t**HYPERPARAMS**\n",
      "KernelModel                [alpha=0.022025, kernel=rbf, gamma=0.067990]                                                                 \n",
      "SVM                        [C=10000.0, kernel=sigmoid, degree=2, gamma=0.067990]                                                        \n",
      "NeuralNetwork              [hidden_layer_sizes=100, activation=relu, solver=adam, alpha=8.653013, learning_rate=invscali, max_iter=1000]\n",
      "LogisticRegressionModel    [solver=liblinea, random_state=0, penalty=l2, tol=0.0001, C=78.47599]                                        \n",
      "Perceptron                 [loss=perceptr, penalty=l2, alpha=0.022025, learning_rate=invscali, eta0=1]                                  \n",
      "GenerativeModel            []                                                                                                           \n",
      "Name: Hyperparams, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pprintBestResults(sortedClassifiersBestPrecision, 'Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t**RESULTS**\n",
      "                         PreProcessing1 PreProcessing2 Recall\n",
      "KernelModel              Normalize       LDA            0.984\n",
      "SVM                      Normalize       LDA            0.979\n",
      "LogisticRegressionModel  Normalize       LDA            0.977\n",
      "NeuralNetwork            Normalize       LDA            0.976\n",
      "Perceptron               StandardScaler  LDA            0.922\n",
      "GenerativeModel          StandardScaler  LDA            0.779\n",
      "\n",
      "\t\t**HYPERPARAMS**\n",
      "KernelModel                [alpha=0.000242, kernel=rbf, gamma=0.007134]                                                                 \n",
      "SVM                        [C=10000.0, kernel=sigmoid, degree=2, gamma=0.067990]                                                        \n",
      "LogisticRegressionModel    [solver=liblinea, random_state=0, penalty=l2, tol=0.0001, C=78.47599]                                        \n",
      "NeuralNetwork              [hidden_layer_sizes=100, activation=relu, solver=adam, alpha=2.803105, learning_rate=invscali, max_iter=1000]\n",
      "Perceptron                 [loss=perceptr, penalty=l2, alpha=0.022025, learning_rate=invscali, eta0=1]                                  \n",
      "GenerativeModel            []                                                                                                           \n",
      "Name: Hyperparams, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pprintBestResults(sortedClassifiersBestRecall, 'Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t**RESULTS**\n",
      "                         PreProcessing1 PreProcessing2 F1Score\n",
      "KernelModel              Normalize       LDA            0.984 \n",
      "SVM                      Normalize       LDA            0.980 \n",
      "NeuralNetwork            Normalize       LDA            0.977 \n",
      "LogisticRegressionModel  Normalize       LDA            0.977 \n",
      "Perceptron               StandardScaler  LDA            0.928 \n",
      "GenerativeModel          StandardScaler  LDA            0.832 \n",
      "\n",
      "\t\t**HYPERPARAMS**\n",
      "KernelModel                [alpha=0.000242, kernel=rbf, gamma=0.007134]                                                                 \n",
      "SVM                        [C=10000.0, kernel=sigmoid, degree=2, gamma=0.067990]                                                        \n",
      "NeuralNetwork              [hidden_layer_sizes=100, activation=relu, solver=adam, alpha=8.653013, learning_rate=invscali, max_iter=1000]\n",
      "LogisticRegressionModel    [solver=liblinea, random_state=0, penalty=l2, tol=0.0001, C=78.47599]                                        \n",
      "Perceptron                 [loss=perceptr, penalty=l2, alpha=0.022025, learning_rate=invscali, eta0=1]                                  \n",
      "GenerativeModel            []                                                                                                           \n",
      "Name: Hyperparams, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pprintBestResults(sortedClassifiersBestF1Score, 'F1Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle finale choisie :\n",
    "\n",
    "Classifier: **KernelModel** \n",
    "\n",
    "**Hyperparameters:**\n",
    "\n",
    "    alpha=0.000, \n",
    "\n",
    "    kernel=rbf,\n",
    "\n",
    "    gamma=0.007\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline': {'DataPreProcessingParams': {'seed': 16082604,\n",
       "   'cases': [{'method': 'Normalize', 'hyperparams': {}},\n",
       "    {'method': 'LDA', 'hyperparams': {'n_components': 100}}]},\n",
       "  'ClassificationParams': {'classifier': 'KernelModel',\n",
       "   'alpha': 0.00024255310558892541,\n",
       "   'kernel': 'rbf',\n",
       "   'gamma': 0.0071349432492320355},\n",
       "  'StatisticsParams': ['Accuracy', 'Precision', 'Recall']},\n",
       " 'results': {'Accuracy': '0.984',\n",
       "  'Precision': '0.985',\n",
       "  'Recall': '0.984',\n",
       "  'F1Score': '0.984'}}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedClassifiersBestF1Score['KernelModel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results sur le Test Set\n",
    "For the best F1-Score result of each classifier type, we will be running on the test set and printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Accuracy  Precision  Recall  F1Score\n",
      "LogisticRegressionModel  0.996     0.996      0.998   0.996  \n",
      "SVM                      0.992     0.989      0.982   0.985  \n",
      "KernelModel              0.992     0.986      0.985   0.985  \n",
      "NeuralNetwork            0.984     0.980      0.982   0.980  \n",
      "Perceptron               0.952     0.947      0.951   0.948  \n",
      "GenerativeModel          0.875     0.897      0.890   0.893  \n"
     ]
    }
   ],
   "source": [
    "testSetResultsClassifiers = {}\n",
    "for classifier in classifiers:\n",
    "    testSetResults = src.Manager.runTestSet(**sortedClassifiersBestF1Score[classifier]['pipeline'])\n",
    "    precision = float(testSetResults['metrics']['Precision'])\n",
    "    recall = float(testSetResults['metrics']['Recall'])\n",
    "    if(recall == 0.0 or precision == 0.0):\n",
    "        testSetResults['metrics']['F1Score'] = 0.0\n",
    "    else:\n",
    "        testSetResults['metrics']['F1Score'] = str(2.0 / (1.0 / precision + 1.0 / recall))[:5]\n",
    "    testSetResultsClassifiers[classifier] = testSetResults\n",
    "\n",
    "#Printing sorted results\n",
    "resultsTestSet = pd.DataFrame(np.zeros((6, 4)), columns = ['Accuracy','Precision','Recall','F1Score'],index = classifiers)\n",
    "for classifier in sorted(testSetResultsClassifiers, key = lambda x:testSetResultsClassifiers[x]['metrics']['F1Score'],reverse = True): \n",
    "    for metric in testSetResultsClassifiers[classifier]['metrics']:\n",
    "        resultsTestSet.loc[classifier][metric] = testSetResultsClassifiers[classifier]['metrics'][metric]\n",
    "print(resultsTestSet.sort_values('F1Score',axis=0,ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrice de confusion avec le meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating Confusion matrix\n",
    "testSetResults = src.Manager.runTestSet(**sortedClassifiersBestF1Score['KernelModel']['pipeline'])\n",
    "predictions = testSetResults['predictions']\n",
    "truths = testSetResults['truths']\n",
    "sns.heatmap(confusion_matrix(truths,predictions))\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('Truths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le diagonal de la matrice de confusion est clair, ce qui montre qu'on eu de très bon résultats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
